<!DOCTYPE HTML>
<html class="no-js" lang="zh-CN">
<head>
    <!--[if lte IE 9]>
<meta http-equiv="refresh" content="0;url=https://www.lutann.com/warn.html">
<![endif]-->
<meta charset="utf-8">
<meta http-equiv="X-DNS-Prefetch-Control" content="on">
<link rel="dns-prefetch" href="https://www.lutann.com">
<link rel="dns-prefetch" href="//www.google-analytics.com">
<link rel="prefetch" href="https://www.lutann.com">
<link rel="prefetch" href="//www.google-analytics.com">


<link rel="prerender" href="https://www.lutann.com">

<meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests">

<meta http-equiv="X-UA-Compatible" content="IE=Edge">
<meta name="renderer" content="webkit">
<meta name="viewport" content="width=device-width, initial-scale=1.0,user-scalable=no">
<meta http-equiv="mobile-agent" content="format=html5; url=https://www.lutann.com">
<meta name="author" content="Lu Tan">

<link rel="stylesheet" href="/css/JSimple.css">


<link rel="shortcut icon" href="/images/favicon1.png">


<title>interview0503 - LuLu&#39;s Blog</title>

<meta name="keywords" content="">

<meta name="description " content="">

    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                processEscapes: true
            }
        });
    </script>


    

    

<meta name="generator" content="Hexo 4.2.0"></head>
<body>
<div id="nav">
    <nav class="nav-menu">
        <a class="site-name current" href="/" title="Lu">Lu</a>
        <a class="site-index current" href="/"><i class="fa fa-home"></i><span>Home</span></a>
        <a href="/archives" title="Archives"><i class="fa fa-archives"></i><span>Archives</span></a>
        <a href="/tags" title="Tags"><i class="fa fa-tags"></i><span>Tags</span></a>
        <!-- custom single page of menus -->
        
        
        <a href="/help" title="帮助">
            <i class="fa fa-question-circle"></i>
            <span>帮助</span>
        </a>
        
    </nav>
</div>

<div class="nav-user">
    <a class="btn-search" href="#"><i class="fa fa-search"></i></a>
    <a class="btn-read-mode" href="#"><i class="fa fa-sun-o"></i></a>
    <a class="btn-sns-qr" href="javascript:"><i class="fa fa-telegram"></i></a>
</div>

<div id="wrapper" class="clearfix">
    <div id="body">
        <div class="main" id="main">
            <div id="cover">
    <div class="cover-img"></div>
    <div class="cover-info">
        
        <h1 class="cover-siteName">LuLu&#39;s Blog</h1>
        <h3 class="cover-siteTitle">Stay hungry, stay foolish.</h3>
        <p class="cover-siteDesc">You can&#39;t connect the dots looking forward; you can only connect them looking backwards. So you have to trust that the dots will somehow connect in your future.</p>
        <div class="cover-sns">
            
    &nbsp;&nbsp;<div class="btn btn-github">
        <a href="https://github.com/Lu-Tan" target="_blank" title="github" ref="friend">
            <i class="fa fa-github"></i>
        </a>
    </div>

    &nbsp;&nbsp;<div class="btn btn-twitter">
        <a href="https://www.zhihu.com/people/tan-lu-50-39" target="_blank" title="twitter" ref="friend">
            <i class="fa fa-twitter"></i>
        </a>
    </div>


        </div>
    </div>
</div>

            <div class="page-title">
    <ul>
        <li><a href="/">Recent Posts</a></li>
        
        
        
        <li class="page-search">
    <form id="search" class="search-form">
        <input type="text"
               readonly="readonly"
               id="local-search-input-tip"
               placeholder="click to search..." />
        <button type="button" disabled="disabled" class="search-form-submit"><i class="fa fa-search"></i></button>
    </form>
</li>

    </ul>
</div>
<div class="main-inner">
    <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
        <div class="post-header">
            <div class="post-author clearfix">
                <a class="avatar fleft" href="https://www.lutann.com"
                   target="_blank">
                    <img width="48" src="/images/favicon1.png" alt="avatar"/>
                </a>
                <p><span class="label">Author</span>
                    <a href="https://www.lutann.com"
                       target="_blank">Lu Tan</a>
                    <span title="Last edited at&nbsp;2020-05-03">2020-05-03</span>
                </p>
                <p>一只小可爱</p>
            </div>
            <h2 class="post-title">interview0503</h2>
            <div class="post-meta">
                emm... 9019 words in the article |
                you are the&nbsp;<span id="busuanzi_value_page_pv"><i class="fa fa-spinner fa-spin"></i></span>th friend who reading now
            </div>
        </div>
        <div class="post-content markdown-body">
            <h1 id="Possible-Questions"><a href="#Possible-Questions" class="headerlink" title="Possible Questions"></a>Possible Questions</h1><h2 id="1-Duration-of-your-summer-internship"><a href="#1-Duration-of-your-summer-internship" class="headerlink" title="1. Duration of your summer internship."></a>1. Duration of your summer internship.</h2><p>‘’’<br>AAAI 截稿时间：9.5<br>ICAART(International Conference on Agents and Artificial Intelligence):10.4<br>ICLR( International Conference on Learning Representations):9.25<br>‘’’</p>
<p>I have plenty of time.<br>I am available from the first of June to the thirtieth of Sep, Since I am preparing for Toefl and GRE in May.<br>I have finished all required courses in Wuhan University, Thus I can continue to work after the thirtieth of Sep, maybe for half of a year.</p>
<p>签证问题：<br>Currently I am still in USA and my visa expires in October. I prefer to work in your lab rather than remote. I will appreciate it if we can make it.</p>
<h2 id="2-Any-potential-research-directions-you-might-be-interested-in"><a href="#2-Any-potential-research-directions-you-might-be-interested-in" class="headerlink" title="2. Any potential research directions you might be interested in?"></a>2. Any potential research directions you might be interested in?</h2><ul>
<li>I am interested in conversitional AI.</li>
<li>3 kinds of problems a dialogue system is expected to solve:<ol>
<li>Question answering: provide concise, direct answers to user queries.</li>
<li>Task completion: range from reservation to meeting scheduling</li>
<li>Social chat: Like a human as in the Turing test. Converse seamlessly and appropriately with users.</li>
</ol>
</li>
</ul>
<h2 id="3-The-NLP-and-HCI-projects-you-mentioned-are-very-different-from-each-other"><a href="#3-The-NLP-and-HCI-projects-you-mentioned-are-very-different-from-each-other" class="headerlink" title="3.The NLP and HCI projects you mentioned are very different from each other."></a>3.The NLP and HCI projects you mentioned are <em>very different</em> from each other.</h2><ul>
<li>Yep, actually there’s no connection between them. Both natural language processing and embedded equipment are of my passion. I plan to be a Phd student and it’s hard to make choice. So I’d like to try both of them to see which one suits me best.</li>
</ul>
<h3 id="NLP-projects"><a href="#NLP-projects" class="headerlink" title="NLP projects"></a>NLP projects</h3><p><strong>1. Intelligent Judge</strong></p>
<ul>
<li><p>Time: summer of twenty nineteen.</p>
</li>
<li><p>Project: Our system is able to <strong>analyze legal information based on semantics</strong> and <strong>make legal predictions from the legal data set</strong>, and hence it helps the judiciary system in automation thereby increasing the efficiency within affordable budget. </p>
</li>
<li><p>My task:</p>
<ul>
<li><ol>
<li>Chinese word segmentation</li>
</ol>
<ul>
<li><p>CRF++ toolkit<br>  CRF++ is an open-source implementation of Conditional Random Fields for segmenting/labeling sequential data.</p>
<ul>
<li><p>Usage<br>Both the training and testing files need to be in a particular format. They must consist of multiple tokens.<br>A token consists of multiple columns. Each token must be represented in one line.<br>Sentence: A sequence of token becomes a sentence. To identify the boundary between sentences, an empty line is put.<br>The last column represents a true answer tag which is going to be trained by CRF.</p>
</li>
<li><p><strong>Steps</strong></p>
</li>
<li><p><strong>step 1</strong>:Make training file and test file in a particular format.</p>
</li>
<li><p><strong>step 2</strong>：Prepare feature templates. This file describes which features are used in training and testing.</p>
<p>  Special macro%x[row,col] will be used to specify a token in the input data. row specfies the relative position from the current focusing token and col specifies the absolute position of the column.</p>
</li>
<li><p><strong>step 3</strong>：Training(encoding)</p>
</li>
<li><p><strong>step 4</strong>: Testing(decoding)</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>Why CRF++?</p>
<ul>
<li><ol>
<li>CRF V.S Dictionary-based word segmentation：<br>Dictionary-based word segmentation relies too much on dictionaries and rule bases, so it has low recognition ability for ambiguous words and unregistered words. But it’s efficicy.</li>
</ol>
<p>CRF not only considers the frequency of the words, but also the context, thus has a good learning ability. Therefore, it performs well at the recognition of ambiguous words and unregistered words; BUT the training period is longer and it needs large amount of calculation.</p>
</li>
<li><ol start="2">
<li>CRF V.S HMM：<br>One of the biggest disadvantages of the hidden Markov model is that due to its output independence assumption, it cannot consider the characteristics of the context, which limits the choice of features.<br>Assumption of HMM:</li>
</ol>
</li>
</ul>
<ol>
<li>p(si|si-1,si-2,…,s1) = p(si|si-1)</li>
<li>P(si+1|si)=p(sj+1,sj) There’s no connection between state and time.</li>
<li>P(o1,…ot|s1,…st) = P(ot|qt) Output is only relavant to current state.</li>
</ol>
</li>
</ul>
<ol start="2">
<li><p>Event extraction<br> Event extraction is the process of gathering knowledge about periodical incidents found in texts, automatically identifying information about <em>what happened and when it happened</em>.</p>
<p> In that project, I am responsible for namekd entity recognition. That’s about extracting named entities such as name, time, location, consequence and so on from legal documents.<br> LSTM-CRF is a state-of-the-art approach to named entity recognition.<br> <img src="https://res.cloudinary.com/dmfrqkuif/image/upload/v1588532677/namekd_cuekww.png" alt=""></p>
</li>
</ol>
<ul>
<li><p>Why we choose CRF+LSTM?</p>
</li>
<li><ol>
<li>Classical Approaches V.S CRF<br>Classical approaches are mostly rule-based.</li>
</ol>
</li>
<li><ol start="2">
<li><p>ML V.S CRF<br>One way of ML: A- treat the problem as a multi-class classification where named entities are our labels so we can apply different classification algorithms.<br><strong>BUT</strong>: This method ignores (1)the thorough understanding of the context of a sentence (2)the sequence of the word labels in it.</p>
<p>Another way: B- is CRF(Conditional Random Field) model. This model is a graphical model, can be used to model sequential data such as labels of words in a sequence.<br><strong>BUT</strong>: Though CRF model is able to capture the features of the current and previou labels in a sequence, it <em>can’t</em> understand the context of the forward labels.</p>
</li>
</ol>
</li>
<li><ol start="3">
<li><p>DL: state-of-the-art</p>
<ul>
<li>Metrics: F1, not accuracy<br>Accuracy: It’s common to use it while training a neural network in different iterations. </li>
</ul>
<p>F1: Because false positives and false negatives have a business cost in a NER task, we use F1 to balance between precision and recall.<br>precision = tp / (tp + fp)<br>recall = tp / (tp + fn)</p>
<ul>
<li>standard LSTM<br>It only take the “past” information in a sequence of the text into account. </li>
</ul>
<p><strong>BUT</strong> in named entity recognition, we need to take both the past and the future labels in sequence.</p>
<ul>
<li>Recurrent neural networks(RNN)<br><img src="https://res.cloudinary.com/dmfrqkuif/image/upload/v1588543758/RNN2_on8ofu.png" alt=""><br><img src="https://res.cloudinary.com/dmfrqkuif/image/upload/v1588543792/rnn3_fatuql.png" alt=""><br>A RNN <strong>maintains a memory based on history information</strong>, which enables the model to predict the current output conditioned on long distance features.</li>
</ul>
<p><img src="https://res.cloudinary.com/dmfrqkuif/image/upload/v1588541643/RNN_tdp8rd.png" alt=""><br>input layer: <em>x</em>: input features. One-hot-encoding. An input layer has the same dimensionality as feature size.</p>
<p>hidden layer <em>h</em>: hidden state. The RNN introduces the connection between the previous hidden state and current hidden state. Thus the recurrent layer weight parameters. The recurrent layer is designed to store history information.<br><strong>h(t) = f( U x(t) + W h(t − 1) )</strong><br>U and W are the connection weights to be computed in training time, f(z) are sigmoid activation function. <img src="https://res.cloudinary.com/dmfrqkuif/image/upload/v1588542223/sigmoid_qdp7g6.png" alt=""></p>
</li>
</ol>
</li>
</ul>
<pre><code> output layer *y*: tags. It presents a probability distribution over labels. It has the same dimensionality as size of labels.
 **y(t) = g( V h(t) )**
 V is connection weight to be computed in training time, g(z) are softmax activation function. ![](https://res.cloudinary.com/dmfrqkuif/image/upload/v1588542335/softmax3_i2eesu.png)

- LSTM Networks
 Long Short Term Memory networks are the same as RNNs, except that **the hidden layer updates are replaced by purpose-built memory cells**. As a result, they may be better at finding and exploiting long range dependencies in the data.
 ![](https://res.cloudinary.com/dmfrqkuif/image/upload/v1588543847/RNN4_ae5jsj.png)
 ![](https://res.cloudinary.com/dmfrqkuif/image/upload/v1588543880/LSTM_fyarr4.png)
 Step by step of LSTM: https://colah.github.io/posts/2015-08-Understanding-LSTMs/

- CRF+LSTM
This network can efficiently use past input features via a LSTM layer and sentence level tag information via a CRF layer.</code></pre><ul>
<li><ol start="3">
<li>Result prediction</li>
</ol>
<ul>
<li>word vector + CNN</li>
</ul>
</li>
</ul>
<p><strong>2. Conversation on movie themes</strong></p>
<ul>
<li><p>Time: Fall of twenty nineteen.</p>
</li>
<li><p>Project: Conversation robot on the topic of movies.</p>
</li>
<li><p>My Task: comparative experiments</p>
<ul>
<li><ol>
<li>Organize the data set into the form required by the model.</li>
</ol>
</li>
<li><ol start="2">
<li>Training, tunning and saving model.</li>
</ol>
</li>
<li><ol start="3">
<li>Comparing performance of Bert, BiDAF, Transformer</li>
</ol>
</li>
</ul>
</li>
<li><p>Bert: Bidirectional Encoder Representations from Transformers<br>  BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.</p>
</li>
<li><p>Transformer<br>Where does it play a role?<br><img src="https://res.cloudinary.com/dmfrqkuif/image/upload/v1588551207/transformer-role_vv3eex.png" alt=""><br>The highlighted words refer to the same person – Griezmann, a popular football player. </p>
<ul>
<li>Seq 2 seq<br> <img src="https://res.cloudinary.com/dmfrqkuif/image/upload/v1588551411/seq2seq_nzlfq2.png" alt=""><br> The task of an encoder network is to understand the input sequence, and create a <em>smaller</em> dimensional representation of it. This representation is then forwarded to a decoder network which generates a sequence of its own that represents the output. <pre><code>- Beam search: It greedily find out the most probable word at each time step.Beam search takes into account the probability of the next k words in the sequence, and then chooses the proposal with the max combined probability.
- attention mechanism: Instead of encoding the input sequence into a single fixed context vector, we let the model learn how to generate a context vector for each output time step. That is we let the model learn what to attend based on the input sentence and what it has produced so far.</code></pre></li>
</ul>
</li>
<li><p>BiDAF(Bi-Directional Attention Flow)<br>   A multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test.</p>
</li>
</ul>
<h3 id="HCI-projects"><a href="#HCI-projects" class="headerlink" title="HCI projects"></a>HCI projects</h3><ul>
<li>Time: Winter of twenty twenty</li>
<li>Project: A contact-based object recognition technique on interactive fabrics using capacitive sensing.</li>
<li>My Task:<ul>
<li><ol>
<li>Design experiments</li>
</ol>
</li>
<li><ol start="2">
<li>Write code</li>
</ol>
</li>
<li><ol start="3">
<li>Write part of the paper.</li>
</ol>
</li>
</ul>
</li>
</ul>

            
                

            
        </div>
        <div class="post-tool">
            <a class="btn-thumbs-up" href="javascript:void(0);" data-cid="52" title="95">
                <i class="fa fa-thumbs-up" aria-hidden="true"></i> Donate
            </a>
        </div>
        
        <div class="post-tags">Tags：
            
        </div>
        
    </article>
    
        <p style="text-align: center">This article just represents my own viewpoint. If there is something wrong, please correct me.</p>
    
    
    

</div>

<script src="/js/busuanzi.pure.mini.js"></script>



        </div><!-- end #main-->
    </div><!-- end #body -->
    <footer class="footer">
    <div class="footer-inner" style="text-align: center">
        <p>
            <a href="/about"  title="About">About</a>&nbsp;&nbsp<em>·</em>&nbsp;&nbsp
            <!-- 自定义链接 -->
            <a href="/help" title="Help" >Help</a>&nbsp;&nbsp<em>·</em>&nbsp;&nbsp
            <a href="/links" title="Links">Links</a>&nbsp;&nbsp<em>·</em>&nbsp;&nbsp
            <a href="/sitemap.xml" title="SiteMap">SiteMap</a>
        </p>
        <p>
            Has been established&nbsp<a href="/timeline" id="siteBuildingTime"></a>&nbspDays，<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="licence">Based on Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)</a><br/>
            ©2017-<span id="cpYear"></span> Based on&nbsp<a href="http://hexo.io" target="_blank" rel="nofollow">Hexo</a>
            ，Theme by&nbsp&nbsp<a href="https://github.com/tangkunyin/hexo-theme-jsimple" target="_blank" rel="bookmark">JSimple</a>
            ，Author&nbsp<a href="https://www.lutann.com" target="_blank" rel="friend">Lu Tan</a>
            ，Hosted by <a href="https://pages.github.com/" target="_blank" rel="nofollow">GitHub Pages</a>
        </p>
    </div>
</footer>

<script src="/js/SimpleCore.js"></script>


</div>
<!-- search pop -->
<div class="popup search-popup local-search-popup">
    <div class="local-search-header clearfix">
        <span class="search-icon">
            <i class="fa fa-search"></i>
        </span>
        <span class="popup-btn-close">
            <i class="fa fa-times-circle"></i>
        </span>
        <div class="local-search-input-wrapper">
            <input id="local-search-input"
                   spellcheck="false"
                   type="text"
                   autocomplete="off"
                   placeholder="Input query keywords here..."/>
        </div>
    </div>
    <div id="local-search-result"></div>
</div>
<div class="fixed-btn">
    <a class="btn-gotop" href="javascript:"> <i class="fa fa-angle-up"></i></a>
</div>
<script>
    $(function () {
        var jsi_config = {
            buildingTime: '04/24/2020',
            current: $('.post-tags').length > 0 ? 'post' : 'archive',
            snsQRCode: '/images/sns-qrcode.png',
            donateImg: '/images/donate-qr.png',
            localSearch: { dbPath: '' },
            readMode: 'day'
        };
        
            jsi_config.localSearch = {
                dbPath: '/search.xml',
                trigger: 'auto',
                topN: '1',
                unescape: 'false'
            }
        
        SimpleCore.init(jsi_config);
        
    });
</script>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true},"react":{"opacity":0.7},"log":false});</script></body>
</html>
