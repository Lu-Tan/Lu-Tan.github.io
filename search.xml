<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Hello World</title>
    <url>/2020/04/25/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>Neural Approaches to Conversational AI</title>
    <url>/2020/04/26/Neural-Approaches-to-Conversational-AI/</url>
    <content><![CDATA[<h3 id="What’s-new"><a href="#What’s-new" class="headerlink" title="What’s new?"></a>What’s new?</h3><ol>
<li><strong>We group conversational systems into three categories: (1) question answering agents, (2) task-oriented dialogue agents, and (3)chatbots, with a unified view of optimal decision making</strong></li>
<li><strong>Draw connections between neural approaches to traditional ones</strong></li>
<li><strong>Present approaches to training dialogue agents using both supervised and reinforcement(++) learning.</strong></li>
<li><strong>Progress and challenges both in research community and industry.</strong></li>
</ol>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><ul>
<li><p>Why we get promising results?</p>
<ol>
<li>large amounts of data available for training.</li>
<li>breakthroughs in deep learning and reinforcement learning.</li>
</ol>
</li>
<li><p>A typical task-oriented dialogue agent is composed of four modules:<br>  <img src="https://res.cloudinary.com/dmfrqkuif/image/upload/v1587927050/1111_zxlmue.png" alt=""></p>
<ol>
<li>NLU(Understanding): identify user intents, extract associate info.</li>
<li>State Tracker: capture all essential info, track the dialogue state.</li>
<li>Dialogue Policy: select the next action based on the current state.</li>
<li>NLG(Generation): convert agent actions to natural language responses.</li>
</ol>
<ul>
<li>But recent trend: Develope fully data-driven systems by unifying these modules using a deep neural network that maps the user input to the agent output DIRECTLY.</li>
</ul>
</li>
<li><p>Difference in modular or not:</p>
<ul>
<li>Task-oriented bots:<ul>
<li>access to an external database</li>
<li>using a modular system</li>
</ul>
</li>
<li>Social chatbots:<ul>
<li>To be AI companions to humans with an emotional connection</li>
<li>mimic human conversation by training DNN-based response generation models on large amounts of human-human conversational data.</li>
<li>non-modular system</li>
</ul>
</li>
</ul>
</li>
<li><p>Dialogue can be formulated as a decision making process.</p>
<ul>
<li>hierarchy<ul>
<li>top level: selects what agent be active</li>
<li>low level: chooses primitive actions to complete the subtask</li>
</ul>
</li>
<li>Hierachical decision making processes can be cast in options over Markov Decision Processes(Sutton et al)<ul>
<li>This view has already been applied to some large-scale open-domain dialogue systems.</li>
</ul>
</li>
<li>If we view each option as an action, both top and low level processes can be natually captured by the reinforcement learning framework.<ul>
<li>Reinforcement learning is an area of machine learning concerned with how software agents ought to take actions in an environment in order to maximize the notion of cumulative reward. Reinforcement learning is one of three basic machine learning paradigms, alongside supervised learning and unsupervised learning.</li>
<li>The goal of dialogue learning is to find optimal policies to maximize expected rewards.</li>
<li>rewards functions:<br><img src="https://res.cloudinary.com/dmfrqkuif/image/upload/v1587945852/222_ctrk2x.png" alt=""></li>
<li><strong>controdictary rewards CPS for xiaoice?</strong><ul>
<li>Although incorporating many task-oriented and QA skills can reduce CPS in the short term since these skills help users accomplish tasks more efficiently by minimizing CPS, these new skills establish XiaoIce as an efficient and trustworthy personal assistant, thus strengthening the emotional bond with human users in the long run.</li>
</ul>
</li>
<li>limit of RL view:<ul>
<li>applying RL requires training the agents by interacting with real users, which can be expensive in many domains.</li>
<li>solution<ul>
<li>hybrid approach: combines the strengths of different ML methods.</li>
<li>For example, we might use imitation and/or supervised learning methods (if there is a large amount of human-human conversational corpus) to obtain a reasonably good agent before applying RL to continue improving it.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Symbolic V.S Neural approaches<br>  <img src="https://res.cloudinary.com/dmfrqkuif/image/upload/v1587950076/333_jcwtrl.png" alt=""></p>
<ul>
<li><p><strong>neural approaches</strong>:</p>
<ul>
<li><strong>can be trained in an end-to-end fasion</strong></li>
<li><strong>robust to paraphrase alternations(detecting and generating paraphrases)</strong></li>
<li><strong>weak in terms of execution efficiency and explicit interpretability</strong></li>
</ul>
</li>
<li><p><strong>symbolic approaches</strong></p>
<ul>
<li><strong>sensitive to paraphrase alternations</strong></li>
<li><strong>more interpretable and efficient in execution.</strong></li>
<li><strong>difficult to train</strong></li>
</ul>
</li>
<li><p>Symbolic approaches</p>
<ul>
<li>dominated for decades</li>
<li>resolve natural language ambiguity at different levels by mapping (or generating) a natural language sentence to (or from) a series of human-defined, unambiguous, symbolic representations, such as Part-Of-Speech (POS) tags, context free grammar, first-order predicate calculus.</li>
<li>now: have been adapted as a rich source of engineered features to be fed into a variety of machine learning models</li>
</ul>
</li>
<li><p>neural approaches</p>
<ul>
<li>do not rely on any human-defined symbolic representations</li>
<li>learn in a task- specific neural space where task-specific knowledge is implicitly represented as semantic concepts using low-dimensional continuous vectors.</li>
<li>steps:<ol>
<li>encoding symbolic user input and knowledge into their neural sematic representations( represent as vectors)</li>
<li>reasoning: to generate a system response based on input and system state.</li>
<li>decoding</li>
</ol>
<ul>
<li>the above three steps are stacked into a deep neural network trained in an end-to-end fastionl via back propagation.<ul>
<li>back propagation:<ul>
<li>a widely used algorithm in training feedforward neural networks for supervised learning. </li>
<li>In fitting a neural network, backpropagation computes the gradient of the loss function with respect to the weights of the network for a single input–output example, and does so efficiently, unlike a naive direct computation of the gradient with respect to each weight individually.</li>
<li>The backpropagation algorithm works by computing the gradient of the loss function with respect to each weight by the chain rule, computing the gradient one layer at a time, iterating backward from the last layer to avoid redundant calculations of intermediate terms in the chain rule; this is an example of dynamic programming.</li>
</ul>
</li>
<li>end-to-end causes: the focus has shifted to carefully tailoring the increasingly complex architecture of neural networks to the end application.</li>
</ul>
</li>
</ul>
</li>
<li>review: Although neural approaches have already been widely adopted in many AI tasks, including image processing, speech recognition and machine translation (e.g., Goodfellow et al., 2016), their impact on conversational AI has come somewhat more slowly.</li>
<li>addition merits: neural approaches provide a consistent representation for many modalities, capturing linguistic and non-linguistic (e.g., image and video (Mostafazadeh et al., 2017)) features in the same modeling framework.</li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>Machine Learning Basics for Conversational AI</title>
    <url>/2020/04/27/Machine-Learning-Basics/</url>
    <content><![CDATA[<h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><p><strong>Mitchell</strong> defines: any computer program that improves its performance at some task T, measured by P, through experiences E.</p>
<ul>
<li>T:(in conversational AI: perform conversations with a user to fulfill the user’s goal.</li>
<li>P: cumulative reward<br><img src="https://res.cloudinary.com/dmfrqkuif/image/upload/v1587945852/222_ctrk2x.png" alt=""></li>
<li>E: a set of dialogues</li>
</ul>
<h2 id="Surpervised-Learning"><a href="#Surpervised-Learning" class="headerlink" title="Surpervised Learning"></a>Surpervised Learning</h2><h4 id="A-common-recipe-of-building-an-ML-agent-using-supervised-learning-SL-consists-of"><a href="#A-common-recipe-of-building-an-ML-agent-using-supervised-learning-SL-consists-of" class="headerlink" title="A common recipe of building an ML agent using supervised learning (SL) consists of"></a>A common recipe of building an ML agent using supervised learning (SL) consists of</h4><ul>
<li>a dataset </li>
<li>The dataset consists of (x, y∗) pairs, where for each input x, there is a ground-truth output y∗. In QA, x consists of an input question and the documents from which an answer is generated, and y∗ is the desired answer provided by a knowledgeable external supervisor</li>
<li>a model</li>
<li>a cost function (a.k.a. loss function)<ul>
<li>The cost function is of the form L(y∗, f(x; θ))</li>
<li>L(.) is often designed as a smooth function(differentiable everywhere) of error.</li>
<li>A commonly used cost function that meets these criteria is the mean squared error (MSE)<br><img src="https://res.cloudinary.com/dmfrqkuif/image/upload/v1588012080/de_bccc3c.png" alt=""></li>
</ul>
</li>
<li>an optimization procedure<ul>
<li>The optimization can be viewed as a search algorithm to identify the best θ that <strong>minimize L(.)</strong>.<br>Given that L is differentiable, the most widely used optimization procedure for deep learning is mini-batch Stochastic Gradient Descent (SGD) which updates θ after each batch as <img src="https://res.cloudinary.com/dmfrqkuif/image/upload/v1588012262/SGD_yin3qj.png" alt=""> where N is the batch size and α the learning rate.</li>
</ul>
</li>
</ul>
<h4 id="Common-supervised-learning-metrics"><a href="#Common-supervised-learning-metrics" class="headerlink" title="Common supervised learning metrics"></a>Common supervised learning <strong>metrics</strong></h4><p>regression problems: mean squared error</p>
<ul>
<li><img src="https://res.cloudinary.com/dmfrqkuif/image/upload/v1588046494/MSE_uncxsu.png" alt=""></li>
</ul>
<p>classification problems</p>
<ul>
<li>binary classification: accuracy, precision, recall, F1 Score<br><img src="https://res.cloudinary.com/dmfrqkuif/image/upload/v1588048090/binary-classification-metrics_kefszn.png" alt=""></li>
<li>beyond binary: BLEU score</li>
</ul>
<h2 id="Reinforcement-Learning"><a href="#Reinforcement-Learning" class="headerlink" title="Reinforcement Learning"></a>Reinforcement Learning</h2><p>Define: In unexplored territories, the agent has to learn how to act by interacting with an unknown environment on its own. </p>
<h4 id="RL-V-S-SL"><a href="#RL-V-S-SL" class="headerlink" title="RL V.S SL:"></a>RL V.S SL:</h4><p>While SL learns from previous experiences provided by a knowledgeable external supervisor, RL learns by experiencing on its own.</p>
<h4 id="Feature-of-RL"><a href="#Feature-of-RL" class="headerlink" title="Feature of RL:"></a>Feature of RL:</h4><ol>
<li>Exploration-exploitation tradeoff. (???)</li>
</ol>
<ul>
<li>exploit: The agent has to exploit what it already knows in order to obtain high rewards.</li>
<li>explore: The agent has to explore unknown states and actions in order to make better action selections in the future.</li>
</ul>
<ol start="2">
<li><p>Delayed reward and temporal credit assignment.<br>The agent has to determine which of the actions in its sequence are to be credited with producing the eventual reward.</p>
</li>
<li><p>Partially observed states.<br>In many RL problems, the observation perceived from the environment at each step, e.g., user input in each dialogue turn, provides only partial information about the entire state of the environment based on which the agent selects the next action.<br>Neural approaches learn a deep neural network to represent the state by <strong>encoding all information</strong> observed at the current and past steps.</p>
</li>
</ol>
<h4 id="A-central-challenge-in-both-RL-and-SL-generalization"><a href="#A-central-challenge-in-both-RL-and-SL-generalization" class="headerlink" title="A central challenge in both RL and SL: generalization"></a>A central challenge in both RL and SL: <strong>generalization</strong></h4><p>the ability to perform well on unseen inputs.</p>
<ul>
<li>solution: neural approaches provide a potentially more effective solution by <strong>leveraging the representation</strong>(?) learning power of deep neural networks.</li>
</ul>
<h1 id="Deep-Learning"><a href="#Deep-Learning" class="headerlink" title="Deep Learning"></a>Deep Learning</h1><h4 id="Deep-learning-V-S-neural-network"><a href="#Deep-learning-V-S-neural-network" class="headerlink" title="Deep learning V.S neural network?"></a>Deep learning V.S neural network?</h4><p>DL involves training neural networks.</p>
<p><strong>Why Deep?</strong></p>
<p>The neural networks, in their original form, consisted of a single layer(i.e., the perceptron).</p>
<p>The perceptron is incapable of learning even simple functions such as logical XOR.</p>
<p>To solve the problem, we add hidden layers(<strong>Why hiden?</strong>) between input and output. –this is called MLP(multi-layer perceptron) or DNN(deep neural networks).</p>
<h2 id="commonly-used-DNNs-for-NLP-amp-IR"><a href="#commonly-used-DNNs-for-NLP-amp-IR" class="headerlink" title="commonly used DNNs for NLP &amp; IR"></a>commonly used DNNs for NLP &amp; IR</h2><h3 id="Softmax-function"><a href="#Softmax-function" class="headerlink" title="Softmax function"></a><strong>Softmax function</strong></h3><p>An activation function.</p>
<p>It outputs a vector that represents the probability distributions of a list of potential outcomes.</p>
<h3 id="normalization-step-taking-exponentials-sums-and-division"><a href="#normalization-step-taking-exponentials-sums-and-division" class="headerlink" title="normalization step: taking exponentials, sums and division."></a><strong>normalization step: taking exponentials, sums and division.</strong></h3><p>ex. Softmax function turns logits [2.0, 1.0, 0.1] into probabilities [0.7, 0.2, 0.1], and the probabilities sum to 1.<br><img src="https://res.cloudinary.com/dmfrqkuif/image/upload/v1588104301/softmax_wy6yoa.png" alt=""></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">logits &#x3D; [2.0, 1.0, 0.1]</span><br><span class="line">exps &#x3D; [np.exp(i) for i in logits]</span><br><span class="line">sum_of_exps &#x3D; sum(exps)</span><br><span class="line">softmax &#x3D; [j&#x2F;sum_of_exps for j in exps]</span><br><span class="line">print(&quot;softmax:&#123;&#125;&quot;.format(softmax))</span><br></pre></td></tr></table></figure>

<p><em>Why we need exponents?</em></p>
<ul>
<li>Logits ranges from negative infinity to positive infinity. When logits are negative, adding it together does not give us the correct normalization. Exponentiate logitsturn them them zero or positive!</li>
</ul>
<p><em>Why special number e?</em></p>
<ul>
<li>e exponents also makes the math easier later! log(a*b)= log(a)+log(b)</li>
</ul>
<p><em>logits layer means the last neuron layer of neural network for classification task which produces raw prediction values.</em></p>
<p><em>Logits: numeric output of the last linear layer of a multi-class classification neural network. Before activation takes place.</em></p>
<p>Softmax function is <em>frequently appended to</em> the last layer of an image classification network such as cnn.</p>
<ul>
<li><img src="https://res.cloudinary.com/dmfrqkuif/image/upload/v1588105160/soft2_a3nsm6.png" alt=""></li>
</ul>
<h2 id="classical-ML-V-S-DL"><a href="#classical-ML-V-S-DL" class="headerlink" title="classical-ML V.S DL"></a>classical-ML V.S DL</h2><p><em>Ex: text classification</em></p>
<h3 id="classical-ML"><a href="#classical-ML" class="headerlink" title="classical ML:"></a>classical ML:</h3><ol>
<li>Map a text string to a vector representation <strong>x</strong>, using a set of <strong>hand-engineered</strong> features.</li>
<li>Learn a linear classifier with a softmax layer to compute the distribution.</li>
</ol>
<p><em>Design effort: feature engineering</em></p>
<h3 id="DL"><a href="#DL" class="headerlink" title="DL"></a>DL</h3><p>Jointly optimize the <em>feature representation</em> and <em>classification</em> using a DNN.</p>
<p><img src="https://res.cloudinary.com/dmfrqkuif/image/upload/v1588128439/ml-vs-dl_nel4rq.png" alt=""></p>
<p>DNN consists of two halves:</p>
<ol>
<li>top half: linear classifier, similar to classical ML.</li>
<li>The input vector of top half is <strong>not from hand-engineered features</strong>, but learned using the bottom half of the DNN.</li>
</ol>
<p><em>Design Effort: optimize DNN architectures for effective representation learning.</em></p>
<h2 id="Which-NN-neural-network-to-choose"><a href="#Which-NN-neural-network-to-choose" class="headerlink" title="Which NN(neural network) to choose?"></a>Which NN(neural network) to choose?</h2><p><strong>Depend on the type of linguistic structures that we hope to capture in the text.</strong></p>
<ol>
<li><p>Option 1: Word Embedding Layers</p>
<p>Map each word to a m-dimensional real-valued vector.</p>
</li>
<li><p>Option 2： Fully Connected Layers</p>
</li>
</ol>
]]></content>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>[BASIC]Word Embedding</title>
    <url>/2020/04/30/BASIC-Word-Embedding/</url>
    <content><![CDATA[<h1 id="Word-Embedding-layers"><a href="#Word-Embedding-layers" class="headerlink" title="Word Embedding layers"></a>Word Embedding layers</h1><h2 id="Define-word-embedding"><a href="#Define-word-embedding" class="headerlink" title="Define word embedding"></a>Define word embedding</h2><p>Word embeddings are a type of word representation that allows words with similar meaning to have a similar representation.</p>
<p>Word embeddings are in fact a class of techniques where individual words are represented as real-valued vectors in a predefined vector space. Each word is mapped to one vector and the vector values are learned in a way that resembles a neural network, <strong>based on the usage of the words.</strong></p>
<p>Each word is represented as a <strong>one hot</strong> vector, whose dimensionality <em>n</em> is the size of a pre-defined voca<br>bulary. The vocabulary is often too large. Word embedding model is to map each one-hot vector to a m-dimensional real-valued vector. m 《 n. The number of features is much smaller than the size of the vocabulary.</p>
<h3 id="one-hot"><a href="#one-hot" class="headerlink" title="one-hot"></a>one-hot</h3><p>EX1: Suppose you have ‘flower’ feature which can take values ‘daffodil’, ‘lily’, and ‘rose’. One hot encoding converts ‘flower’ feature to three features, ‘is_daffodil’, ‘is_lily’, and ‘is_rose’ which all are binary.</p>
<p>EX2: <img src="https://res.cloudinary.com/dmfrqkuif/image/upload/v1588178728/one-hot_qa7mmr.png" alt=""></p>
<p>Word embeddng is one of the key breakthroughs of DL on challenging NLP.  </p>
<ol>
<li><strong>low-dimensinal vector</strong>: the majority of neural network toolkits do not play well with very high-dimensional, sparse vectors</li>
<li><strong>dense vector</strong>:  it is worthwhile to provide a representation that is able to capture these similarities on features.</li>
</ol>
<h2 id="Word-embedding-algorithms"><a href="#Word-embedding-algorithms" class="headerlink" title="Word embedding algorithms"></a>Word embedding algorithms</h2><p><strong>Word embedding methods are learning process.</strong></p>
<p>They learn a <em>real-valued vector representation</em> for a predefined fixed sized vocabulary <em>from a corpus of text</em>.</p>
<p>Two types of learning process:</p>
<ol>
<li>Joint with neural network model on some task.</li>
<li>Unsupervised process, using document statistics.</li>
</ol>
<p>Three ways to learn a word embedding:</p>
<p><strong>1.Embedding layer</strong></p>
<p>It’s a word embedding, which is learned jointly with a <em>neural network model</em> on a <em>specific</em> natural language processing task(ex: language modeling or document classification).</p>
<p>It’s <em>used</em> on the front end of a neural network and is fit in a supervised way using a Backpropagation algorithm.</p>
<p>It requires that document text be cleaned and prepared such that each word is one-hot encoded.</p>
<ul>
<li>disadvantage: It requires a lot of training data and can be slow</li>
<li>advantage: It will learn an embedding both targeted to the <em>specific text data</em> and the <em>NLP task</em>.</li>
</ul>
<p><strong>2. Word2Vec</strong></p>
<p>It is <strong>effient</strong>, good at capturing syntactic and semantic regularities, learns a <em>standalone</em> word embedding from a text corpus.</p>
<pre><code>low space and time complexity
more dimensions
much larger corpora of text </code></pre><p><strong>Ex: King - man + woman = Queen.</strong></p>
<p>– Word2Vec involved analysis of the learned vectors and the exploration of vector math on the representations of words. </p>
<p><em>Two different models</em>:</p>
<ul>
<li><p>CBOW(continuous Bag-of-Words) model</p>
<p>  CBOW learns the embedding by predicting the current word based on its context.</p>
</li>
<li><p>Continuous Skip-Gram Model</p>
<p>  It learns by predicting the surrounding words given a current word.</p>
</li>
</ul>
<p><img src="https://res.cloudinary.com/dmfrqkuif/image/upload/v1588197753/word2vec_j3hnqr.png" alt=""></p>
<p><strong>Size of sliding winodw</strong><br>The context is defined by a window of neighboring words. This window is a configurable parameter of the model.</p>
<p>Large windows tend to produce more topical similarities, while smaller windows tend to produce more functional and syntactic similarities.</p>
<p><strong>3.GloVe: The Global Vectors for Word Representation</strong></p>
<p>It is an extension to the word2vec method for efficiently learning word vectors. GloVe is an approach to marry both the <em>global statistics of matrix factorization techniques</em> like LSA with the <em>local context-based learning</em> in word2vec.</p>
<ul>
<li>LSA(latent semantic analysis): It’s for analyzing relationships between a set of documents and the terms they contain by <em>producing a set of concepts related to the documents and terms</em>. LSA assumes that <em>words that are close in meaning will occur in similar pieces of text</em>. A matrix containing word counts per document (rows represent unique words and columns represent each document). Documents are then compared by taking the cosine of the angle between the two vectors (or the dot product between the normalizations of the two vectors) formed by any two columns. Values close to 1 represent very similar documents while values close to 0 represent very dissimilar documents</li>
</ul>
<p>Rather than using a window to define local context, <em>GloVe constructs an explicit word-context or word co-occurrence matrix using statistics across the whole text corpus.</em></p>
<p>GloVe performs well at word analogy, word similarity, and named entity recognition tasks.</p>
]]></content>
      <tags>
        <tag>BASIC</tag>
      </tags>
  </entry>
  <entry>
    <title>碎碎念-前途未卜</title>
    <url>/2020/05/20/%E7%A2%8E%E7%A2%8E%E5%BF%B5-%E5%89%8D%E9%80%94%E6%9C%AA%E5%8D%9C/</url>
    <content><![CDATA[<h5 id="我们走出人生中几乎所有关键性步骤时，都是在一种难以觉察的情况下顺应内心的结果。"><a href="#我们走出人生中几乎所有关键性步骤时，都是在一种难以觉察的情况下顺应内心的结果。" class="headerlink" title="我们走出人生中几乎所有关键性步骤时，都是在一种难以觉察的情况下顺应内心的结果。"></a>我们走出人生中几乎所有关键性步骤时，都是在一种难以觉察的情况下顺应内心的结果。</h5><p>回国一周啦，在广州集中隔离。虽说是一个人待在房间里面不让出去，看起来有点惨，但我似乎并没有什么难受的感觉。酒店工作人员都很友善，爸妈给的钱也足够多，每天点的外卖都很好吃（广州居然可以通过外卖点到喜茶！！！），手上也一直都有事情做。过去的一周里，前三四天都在急着看Related Work,十篇左右的密密麻麻的英文论文看的头皮发麻嘤嘤嘤，可能是笔记本的屏幕太小了不舒服吧~因为是做信息抽取，不必自己提出算法改进模型之类的，所以还算好上手，学习难度也不是特别大。整理成文档发给老师后，被夸是Very well-organized，哈哈哈我快乐了。</p>
<p>后面几天花了一天写中华乐教导引的作业，我选的题目是中华乐教在当代的弘扬路径，一不小心写出了一篇习概课文的感觉，果然是多年不写文章、写作能力退化到小学水平。希望以后能抽空多写写博文，练练手也是好的。</p>
<p>考虑到美国受疫情的影响，我开始考虑起保研的事情。之前想着去美国，其实是有保障的。如果读master,那毕业之后可以高薪工作、也可以继续申请phd。如果读phd，五年读出来又有科研实力又有一堆文章和人脉，不论是工作还是回国拿教职都是极好的。但疫情影响美国经济，这样一来读完master可能找不到工作、需要回国996，那何必出国读两年而不留在国内读呢？ Phd的话预计明年会很难申请，如果老师没有funding不招学生，那就没有phd可读。国内保研的话，人多坑少，竞争那叫一个惨烈。抱着买彩票的心态报名了几个夏令营，感谢愿意为我推荐的超级无敌好的文老师和黄老师~</p>
<p>偶尔还是会怀念达特茅斯的蓝天、Teyen老板家门口的Connecticut River、轻轻飘落雪花的Hanover小镇。更怀念在实验室废寝忘食又快乐的干活的自己哈哈哈</p>
<p>关于读研的去向问题，最近问了不少人。参加工作有参加工作的乐趣，也没有想象中的007式的辛苦，工资也比我形象中的高很多；国内读研也有国内的好处，如果跟的是本校最厉害的老师，那硕士期间也会过得很好，老师会支持你的想法，给予很大的帮助；但如果去外校读研，将面临导师很水的风险，本校本科生一直以来都是该校硕士的最好生源，好的老师的名额基本上都会分配给本校的学生，外校学生进去本身就很困难、进去之后能跟一个好的老师就更困难，我认为国内去外校保研是一场赌博，不太想冒这个险。国内只报清北的夏令营，能不能去都随缘；暑假的主要精力还是放在出国的准备方面：做暑研；考语言。最后大概率会是老老实实出国~</p>
<p>虽然疫情的到来导致情况非常复杂，计划赶不上变化，但我也乐于接受变幻莫测的未知的未来、变化未尝不是好事呢？</p>
]]></content>
      <tags>
        <tag>LIFE</tag>
      </tags>
  </entry>
  <entry>
    <title>Survey-NLP-Health</title>
    <url>/2020/05/20/Survey-NLP-Health/</url>
    <content><![CDATA[<h2 id="各篇文章的概述"><a href="#各篇文章的概述" class="headerlink" title="各篇文章的概述"></a>各篇文章的概述</h2><h3 id="Large-scale-Analysis-of-Counseling-Conversations-An-Application-of-Natural-Language-Processing-to-Mental-Health"><a href="#Large-scale-Analysis-of-Counseling-Conversations-An-Application-of-Natural-Language-Processing-to-Mental-Health" class="headerlink" title="Large-scale Analysis of Counseling Conversations: An Application of Natural Language Processing to Mental Health"></a>Large-scale Analysis of Counseling Conversations: An Application of Natural Language Processing to Mental Health</h3><p>自然语言处理在心理健康领域的一个应用：分析大规模的咨询对话。</p>
<p><strong>研究的出发点：</strong></p>
<p>由于过去缺乏大规模的带有标记了的结果的对话数据，我们一直都没搞清楚应该如何引导成功的咨询对话。</p>
<p><strong>我们的努力</strong></p>
<p>我们在以文字为沟通方式的对话数据上，做了大规模的量化的研究。通过计算机来衡量语言学方面是如何影响到对话结果的。</p>
<p><strong>Contribution</strong></p>
<ol>
<li>是目前为止的最大规模的咨询对话策略方面的研究</li>
<li>特别的，我们注重分析咨询师，而不是分析病人。因为我们感兴趣的是通用的对话策略研究、而不是具体某一个话题。</li>
<li>我们找到了切实可行的对话策略：: Adaptability; Dealing with ambiguity; Creativity; Making progress; Change in perspective.</li>
<li>我们证明了对话的结果是可以通过我们发现的那些策略特征来预测的。</li>
</ol>
<p><strong>评价</strong></p>
<p>我挺喜欢这篇文章的，希望我们也可以做出类似的东西。感觉行文非常自然，顺畅。</p>
<h3 id="The-Channel-Matters-Self-disclosure-Reciprocity-and-Social-Support-in-Online-Cancer-Support-Groups"><a href="#The-Channel-Matters-Self-disclosure-Reciprocity-and-Social-Support-in-Online-Cancer-Support-Groups" class="headerlink" title="The Channel Matters: Self-disclosure, Reciprocity and Social Support in Online Cancer Support Groups"></a>The Channel Matters: Self-disclosure, Reciprocity and Social Support in Online Cancer Support Groups</h3><p>channel影响了在线癌症支持小组中人们发朋友圈吐槽和为他人提供支持鼓励的方式</p>
<p><strong>研究的出发点</strong> </p>
<p>我们想知道 之前的对于 人们在通用网站（facebook)上面使用不同的channel进行自我表露（发朋友圈）的方式的 的研究是不是适用于在线健康支持小组中。</p>
<p><strong>我们检测了这些方面</strong> </p>
<ol>
<li>在在线癌症支持小组中，人们使用公开的channel和私密的channel发朋友圈的不同方式。</li>
<li>channel是如何平衡好发朋友圈吐槽和提供支持帮助这两个方面的。</li>
</ol>
<h3 id="Causal-Factors-of-Effective-Psychosocial-Outcomes-in-Online-Mental-Health-Communities"><a href="#Causal-Factors-of-Effective-Psychosocial-Outcomes-in-Online-Mental-Health-Communities" class="headerlink" title="Causal Factors of Effective Psychosocial Outcomes in Online Mental Health Communities"></a>Causal Factors of Effective Psychosocial Outcomes in Online Mental Health Communities</h3><p>在线心理健康社区中，产生有效的社交心理学效果的一些原因</p>
<p><strong>研究的出发点</strong></p>
<p>我们好奇：同伴间的支持（peer support）中的哪些因素造成了有效的社交对话效果？</p>
<p><strong>Contribution</strong></p>
<p>我们使用个例对照法（case-control)来研究是什么因素导致了有效的支持。<br>个例对照法来自于流行病学，它的好处是：不像正因果推论法（ forward causal inference methods）需要把变量二进制化，我们保留原值。这个方法对于①结果是明确定义的②因素是连续变量的分析是很有效的。<br>我们的方法不仅可以同时研究多个因素的影响，还可以研究单个因素在多大程度上有影响。</p>
<h3 id="Trouble-on-the-Horizon-Forecasting-the-Derailment-of-Online-Conversations-as-they-Develop"><a href="#Trouble-on-the-Horizon-Forecasting-the-Derailment-of-Online-Conversations-as-they-Develop" class="headerlink" title="Trouble on the Horizon: Forecasting the Derailment of Online Conversations as they Develop"></a>Trouble on the Horizon: Forecasting the Derailment of Online Conversations as they Develop</h3><p>在对话中的冲突出现之前就预测到即将产生的冲突</p>
<p><strong>研究的出发点</strong></p>
<p>在线的聊天中，大家可能聊着聊着就发生了冲突、争吵、攻击对方。最近别人的研究主要在检测单句话中的引起社交冲突(antisocial bahavior)，在聊天结束之后才来分析。我们想要在对话中出现冲突之前及时预测到即将出现冲突。<br>要实现预测有几个难题：</p>
<ol>
<li>对话是动态的，产生什么结果是取决于后续双方的交流。之前的解决方式是：主要依赖于手工制作的feature来找到冲突。也有人用neural attention的方式来解决。</li>
<li>对话的长度不定，需要预测的可能出现的冲突可能随时出现。之前的工作是：①预测发生冲突的时间，在那个时间到的时候来检查；②从固定大小的窗口中抽取特征。（吐槽点：window-size?)</li>
</ol>
<p><strong>Contribution</strong></p>
<ol>
<li>一个模型：可以在对话发生的过程中实现预测。</li>
<li>通过实时分析各个语句以及他们的关系，克服了那几个难题。</li>
<li><strong><em>在对话预测领域第一个使用pre-train-then-fine-tune方法的。</em></strong></li>
</ol>
<h3 id="Seekers-Providers-Welcomers-and-Storytellers-Modeling-Social-Roles-in-Online-Health-Communities"><a href="#Seekers-Providers-Welcomers-and-Storytellers-Modeling-Social-Roles-in-Online-Health-Communities" class="headerlink" title="Seekers, Providers, Welcomers, and Storytellers: Modeling Social Roles in Online Health Communities"></a>Seekers, Providers, Welcomers, and Storytellers: Modeling Social Roles in Online Health Communities</h3><p>给在线健康社区中的人进行社会角色建模，分为寻求帮助者，提供帮助者，欢迎新人者，讲故事的人。</p>
<p><strong>Contribution</strong></p>
<p>我们从癌症互助社区中，以用户的行为模式为依据，建了11个特定功能的角色模式。<br>我们还研究了角色的动态变化，包括：在用户使用这个社区期间，用户的角色是如何改变的？ 如何根据角色来预测用户是否会长期参与到这个社区中？<br>我们发现，用户频繁的改变自己的角色，从求助者到提供帮助者。整个社区的角色分布还是稳定的。<br>早期就形成特定角色的人在社区中参与时间会更长。</p>
<h3 id="Linguistic-Markers-Indicating-Therapeutic-Outcomes-of-Social-Media-Disclosures-of-Schizophrenia"><a href="#Linguistic-Markers-Indicating-Therapeutic-Outcomes-of-Social-Media-Disclosures-of-Schizophrenia" class="headerlink" title="Linguistic Markers Indicating Therapeutic Outcomes of Social Media Disclosures of Schizophrenia"></a>Linguistic Markers Indicating Therapeutic Outcomes of Social Media Disclosures of Schizophrenia</h3><p>精神分裂症</p>
<p><strong>研究的出发点</strong><br>越来越多的人开始在社交媒体网站上倾诉自己的不开心。我们想知道：</p>
<ol>
<li>在发朋友圈前后，行为有什么变化？</li>
<li>这些行为变化中，包括利于治疗的”opening up”吗？</li>
</ol>
<p><strong>Contribution</strong></p>
<ol>
<li>用量化的方式来找到disclosure前后的时间阶段。</li>
<li>用一系列语言学的指标来描述disclosure前后的行为变化</li>
<li>我们找到了disclosure之后产生疗效的因素：improved readability and coherence in language, future orientation, lower self preoccupation, and reduced discussion of symptoms and stigma perceptions</li>
</ol>
<p><strong>评论</strong><br>这篇文章我也很喜欢。行文顺畅，逻辑很自然。</p>
<h3 id="What-Makes-a-Good-Counselor-Learning-to-Distinguish-between-High-quality-and-Low-quality-Counseling-Conversations"><a href="#What-Makes-a-Good-Counselor-Learning-to-Distinguish-between-High-quality-and-Low-quality-Counseling-Conversations" class="headerlink" title="What Makes a Good Counselor? Learning to Distinguish between High-quality and Low-quality Counseling Conversations"></a>What Makes a Good Counselor? Learning to Distinguish between High-quality and Low-quality Counseling Conversations</h3><p>通过分析好咨询师和坏咨询师的不同点，从而找到如何成为一个好的咨询师的方法</p>
<p><strong>Contribution</strong></p>
<ol>
<li>使用来自public sources 的noisy的咨询数据，来分析咨询质量。（实用，点赞👍）</li>
<li>通过分析对话的各个方面（ turn-by-turn interaction, the sentiment expressed during the interaction, linguistic alignment, and salient topics)，从而找到了高质量咨询的模式。</li>
<li>通过我们分析中找到的特征和标准N-GRAM特征，提升了咨询质量分类器的精度。</li>
</ol>
<h3 id="Moments-of-Change-Analyzing-Peer-Based-Cognitive-Support-in-Online-Mental-Health-Forums"><a href="#Moments-of-Change-Analyzing-Peer-Based-Cognitive-Support-in-Online-Mental-Health-Forums" class="headerlink" title="Moments of Change: Analyzing Peer-Based Cognitive Support in Online Mental Health Forums"></a>Moments of Change: Analyzing Peer-Based Cognitive Support in Online Mental Health Forums</h3><p><strong>研究的出发点</strong><br>重塑不理性的认知可以给心理疾病患者带来积极的认知的改变。我们想知道：在跟朋友的对话中（peer to peer)，这些认知的改变是如何发生的。</p>
<p><strong>我们的努力</strong></p>
<ol>
<li>定义了“a moment of change”.对于一个患者曾经表示负面情绪的话题，只要患者表达了积极的情绪，那我们就称之为”a moment of change”</li>
<li>提出了一个模型，可以预测一个对话或者一个朋友圈（post)中是否有发生“a moment of change”</li>
<li>我们提出的SentiTopic模型可以显式的追踪每个post中的话题和情感。据此我们知道重塑认知是如何发生的。</li>
</ol>
<p><strong>评价</strong></p>
<p>这篇我也喜欢233，实时的追踪话题很有趣</p>
<h3 id="Finding-Your-Voice-The-Linguistic-Development-of-Mental-Health-Counselors"><a href="#Finding-Your-Voice-The-Linguistic-Development-of-Mental-Health-Counselors" class="headerlink" title="Finding Your Voice: The Linguistic Development of Mental Health Counselors"></a>Finding Your Voice: The Linguistic Development of Mental Health Counselors</h3><p>咨询师随着经验的累积 在语言方面的进步</p>
<p><strong>研究的问题</strong></p>
<p>随着经验的累积，咨询师在哪种程度上改变他们的linguistic behavior？这个改变的原因是什么？</p>
<p><strong>我们的工作</strong></p>
<ol>
<li>在两个维度量化了咨询师的变化：咨询师自己；咨询师之间<strong>Contribution</strong>由此我们是第一个证明了咨询师确实会随着经验的累积而进步的。</li>
<li>计算了咨询师的改变的速率</li>
<li>找到了一些有用的可以导致明显的咨询师改变的原因。</li>
</ol>
<p><strong>评价</strong><br>喜欢，“第一个xxx”总给人一种很有用的感觉</p>
<h3 id="The-role-of-conversation-in-health-care-interventions-enabling-sensemaking-and-learning"><a href="#The-role-of-conversation-in-health-care-interventions-enabling-sensemaking-and-learning" class="headerlink" title="The role of conversation in health care interventions: enabling sensemaking and learning"></a>The role of conversation in health care interventions: enabling sensemaking and learning</h3><p>这是一篇纯粹的心理学研究，分析了对话在保健中的作用：enabling sensemaking and learning。并分析了原因<br>Sensemaking就是遇到期望之外的情况时，如何用以前的知识去解决未知的问题。<br>Learning是通过与外界交换信息，从而调整自己的观点和策略。</p>
<h2 id="四个问题的回答"><a href="#四个问题的回答" class="headerlink" title="四个问题的回答"></a>四个问题的回答</h2><h3 id="How-do-we-predict-the-quality-of-therapy-conversations-given-the-survey-like-self-reported-satisfaction-score-among-different-mental-health-conditions"><a href="#How-do-we-predict-the-quality-of-therapy-conversations-given-the-survey-like-self-reported-satisfaction-score-among-different-mental-health-conditions" class="headerlink" title="How do we predict the quality of therapy conversations given the survey-like self-reported satisfaction score among different mental health conditions?"></a>How do we predict the quality of therapy conversations given the survey-like self-reported satisfaction score among different mental health conditions?</h3><p>根据Survey-like的表，如何预测对话的治疗质量？</p>
<p>目前没有人做：ex.5个问题，每个问题评分1-10.<br>目前做的事情：</p>
<ol>
<li>根据语言学方面分析咨询质量</li>
<li>看有没有a moment of change</li>
<li>问咨询者是否感觉受到了帮助。（我认为不提问“是否”，而是询问“程度”会好一些。</li>
</ol>
<h3 id="Given-the-limited-number-of-self-reported-score-how-do-we-build-a-classification-model-that-can-utilize-the-information"><a href="#Given-the-limited-number-of-self-reported-score-how-do-we-build-a-classification-model-that-can-utilize-the-information" class="headerlink" title="Given the limited number of self-reported score, how do we build a classification model that can utilize the information?"></a>Given the limited number of self-reported score, how do we build a classification model that can utilize the information?</h3><p>给定有限数量的question 1中的问卷，我们如何构建分类模型？</p>
<ol>
<li>logitic regression模型来预测。（大规模数据）</li>
<li>pre-train-then-fine-tune（小规模数据）</li>
<li>SVM （support vector machine classifiers)(小规模)</li>
</ol>
<h3 id="How-do-we-discover-the-linguistic-features-that-can-improve-the-quality-of-conversations"><a href="#How-do-we-discover-the-linguistic-features-that-can-improve-the-quality-of-conversations" class="headerlink" title="How do we discover the linguistic features that can improve the quality of conversations?"></a>How do we discover the linguistic features that can improve the quality of conversations?</h3><p>如何找到提升对话质量的语言学特征？</p>
<p>做的人太多了，不是我们的contribution</p>
<h3 id="Are-linguistic-features-differ-among-different-mental-health-conditions-If-so-how-should-we-detect-such-differences"><a href="#Are-linguistic-features-differ-among-different-mental-health-conditions-If-so-how-should-we-detect-such-differences" class="headerlink" title="Are linguistic features differ among different mental health conditions? If so, how should we detect such differences?"></a>Are linguistic features differ among different mental health conditions? If so, how should we detect such differences?</h3><p>不同的心理疾病之间的语言学特征相同吗？不同的话，我们如何找到这些不同？</p>
<p>目前没有人专门研究这个问题，只是零零散散的研究某个方面，比如癌症、精神分裂症。<br>我们可能的contribution:通过查找别人做过的 + 探究别人没做过的 ，做一个心理疾病的汇总工作。</p>
<h2 id="Question"><a href="#Question" class="headerlink" title="Question"></a>Question</h2><ol>
<li>想知道更多有关项目的内容：数据集包含哪些信息、什么格式；我们要做什么事情；我们的Contribution在哪？<br>目前我已知的内容是：有医生和病人之间的1W-2W的数据集，是对话。</li>
<li>时间安排：暑假除了考GT之外，可能会参加一个夏令营，再没了。时间应该足够，希望制定时间计划。<br>调研工作<br>方法的总体安排<br>分部开始执行<br>写论文<br>修改论文 </li>
<li>询问老板对于疫情过后留学申请的看法：疫情对Phd招生的影响；对Master工作的影响。</li>
</ol>
]]></content>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>week1-health</title>
    <url>/2020/05/21/week1-health/</url>
    <content><![CDATA[<h3 id="TASK-1W-2W寻找research-topic"><a href="#TASK-1W-2W寻找research-topic" class="headerlink" title="TASK:1W-2W寻找research topic"></a>TASK:1W-2W寻找research topic</h3><p>在给定的数据集下寻找任何可能的research topic.</p>
<h4 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h4><p>心理疾病：抑郁症，焦躁症。<br>专业的线上抑郁症聊天平台，用户刚进入网站的时候会填一些问题，从而被match一些咨询师。类似于微信一样，患者可以选择同时跟多个人聊天、也可以一直就跟某一个人聊天。聊天的过程中，患者可以like the message(类似于slack)，结束之后患者会填一个evaluation form，说自己是否满意。<br><em>特别的，患者可以选择一直跟某个人聊天</em> ， 这是异于以往任何研究的。我们可以由此添加一个新的Measure:患者下一次是否继续跟某个咨询师聊天。随着聊天的进行，他们的话题、情感都可能会发生变化，可能会更加的亲密。</p>
<h4 id="inspiration：聊的越多越亲密吗？"><a href="#inspiration：聊的越多越亲密吗？" class="headerlink" title="inspiration：聊的越多越亲密吗？"></a>inspiration：聊的越多越亲密吗？</h4><p>做一个模型，分析亲密程度跟聊天次数、聊天时长的关系；分析话题、情感的变化：是否在linguistic feature上面有变化，例如：</p>
<ol>
<li><p>Successful counselors are better at <strong>adapting</strong> to the conversation<br><img src="https://res.cloudinary.com/dmfrqkuif/image/upload/v1590033524/02_q7rtvp.png" alt=""><br>分析方式1：① Look for language differences between positive and negative conversations<br>② Observe how this distance changes over time<br>分析方式2：topical congruence &amp; linguistic style accommodation</p>
</li>
<li><p>More successful counselors react more strongly to <strong>ambiguous</strong> situations than less successful counselors.<br>分析方式： Compare how more-successful and less-successful counselors react given nearly identical situations</p>
</li>
<li><p>More successful counselors use less common/<strong>templated</strong> responses<br>分析方式1：Count the number of similar responses in TF-IDF space for the counselor reaction<br>分析方式2：①Average distance (or diversity) among the responses received(higher, better)<br>②To inspect top keywords in responses received to check if they are generic.</p>
</li>
<li><p>More successful counselors <strong>coordinate less</strong> than less successful ones<br>分析方式：① Divide a conversation into 5 stages with unsupervised model.<br>② Compute the average duration in messages of each stage.<br>③ Explore the reason for step2 by analyzing linguistic coordination.<br>分析方式2：<br>Linguistic Style Matching(LSM) and  linguistic Style Coordination(LSC) method</p>
</li>
<li><p>Counselor actively induce <strong>perspective change: time, self, sentiment</strong><br>分析方式：Time: the relative amount of words in the LIWC past, present, and future categories<br>Self: relative amount of first person singular pronouns (I, me, mine) versus third person singular/plural pronouns (she, her, him / they, their), again using LIWC.<br>Sentiment: the relative fraction of positive words using the LIWC PosEmo and NegEmo sentiment lexicons.</p>
</li>
<li><p>Longer responses and lower <strong>repeatability</strong> of words are more likely to help psychosocial improvement.<br>分析方式：Average length of response &amp; Average number of unique words per response</p>
</li>
<li><p>Emotionality: greater <strong>positivity</strong> is associated with effective.<br>分析方式：无</p>
</li>
<li><p>Credibility of the Responders: responses from members who are more <strong>active on the platform</strong> seem to be typically more effective.<br>分析方式： responders’ tenure (number of days on the platform) (no difference)<br>interactivity<br>number of posts(no difference)<br>the frequency of posting behavior (posts per day)</p>
</li>
<li><p><strong>Average words per turn（ for counselor)，word ratio，sentiment change</strong> of counselor<br>分析方式： Analyze turn-by-turn interaction by calculating each speaker’s average word per turn and word ratio between client and counselor.<br>Divide each session into 5 stages，each stage has similar numbers of turns. We calculate average number of words per stage</p>
</li>
<li><p>relation between <strong>topic</strong> and counseling quality<br>分析方式：① meaning extraction method(MEM)， to get resulting word list.<br>② Generate counselor and client matrices containing binary vectors indicating the use of each word by a specific speaker<br>③ Run a Principal Component Analysis (PCA), followed by varimax rotation on each document matrix to find clus- ters of co-occurring nouns. This process results in 10 and 8 components (topics) for counselors and clients respectively</p>
</li>
<li><p>linguistic diversity<br><img src="https://res.cloudinary.com/dmfrqkuif/image/upload/v1590033397/01_q0krwu.png" alt=""><br>分析方式：STEP 1: Design a general framework aimed at capturing the degree of linguistic diversity.<br>STEP 2: Instantiate this framework in the counseling domain.<br>STEP 3: Estimate the relation between the linguistic diversity of counselors and their effectiveness in engendering positive outcomes.<br>STEP 4: Use the resulting high level linguistic characterization to analyze the evolution. (MAIN FOCUS)</p>
</li>
<li><p>特别的，由于数据集的特别，我们可以添加一个特征：下次是否还继续跟同一个医生聊天。</p>
</li>
</ol>
<h4 id="inspiration：随着聊天越来越多，他们的话题是否变化？情感是否变化？上述的语言feature是否变化？"><a href="#inspiration：随着聊天越来越多，他们的话题是否变化？情感是否变化？上述的语言feature是否变化？" class="headerlink" title="inspiration：随着聊天越来越多，他们的话题是否变化？情感是否变化？上述的语言feature是否变化？"></a>inspiration：随着聊天越来越多，他们的话题是否变化？情感是否变化？上述的语言feature是否变化？</h4><h4 id="inspiration-成功的counselor到底是怎么安慰人的-他们有哪些strategy-我们能否发现更多的strategy"><a href="#inspiration-成功的counselor到底是怎么安慰人的-他们有哪些strategy-我们能否发现更多的strategy" class="headerlink" title="inspiration: 成功的counselor到底是怎么安慰人的? 他们有哪些strategy? 我们能否发现更多的strategy?"></a>inspiration: 成功的counselor到底是怎么安慰人的? 他们有哪些strategy? 我们能否发现更多的strategy?</h4><p>培训咨询师应该培训哪些方面？</p>
<p>需要补一些心理学的知识</p>
<h4 id="分析病人随着待在平台的时间，选择的聊天的人的变化情况。为什么会一直跟某人聊下去"><a href="#分析病人随着待在平台的时间，选择的聊天的人的变化情况。为什么会一直跟某人聊下去" class="headerlink" title="分析病人随着待在平台的时间，选择的聊天的人的变化情况。为什么会一直跟某人聊下去"></a>分析病人随着待在平台的时间，选择的聊天的人的变化情况。为什么会一直跟某人聊下去</h4><h4 id="已公开且允许使用的数据集有哪些？"><a href="#已公开且允许使用的数据集有哪些？" class="headerlink" title="已公开且允许使用的数据集有哪些？"></a>已公开且允许使用的数据集有哪些？</h4><p>RADDIT?</p>
]]></content>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>保研后的生活</title>
    <url>/2020/10/27/%E4%BF%9D%E7%A0%94%E5%90%8E%E7%9A%84%E7%94%9F%E6%B4%BB/</url>
    <content><![CDATA[<h5 id="对生活做种种设置是人特有的品性。不光是设置动物，也设置自己。我们知道，在古希腊有个斯巴达，那里的生活被设置得了无生趣，其目的就是要使男人成为亡命战士，使女人成为生育机器，前者像斗鸡，后者像母猪。这两类动物是很特别的，但我以为，它们肯定不喜欢自己的生活。但不喜欢又能怎么样？人也好，动物也罢，都很难改变自己的命运。-————王小波-《一只特立独行的猪》"><a href="#对生活做种种设置是人特有的品性。不光是设置动物，也设置自己。我们知道，在古希腊有个斯巴达，那里的生活被设置得了无生趣，其目的就是要使男人成为亡命战士，使女人成为生育机器，前者像斗鸡，后者像母猪。这两类动物是很特别的，但我以为，它们肯定不喜欢自己的生活。但不喜欢又能怎么样？人也好，动物也罢，都很难改变自己的命运。-————王小波-《一只特立独行的猪》" class="headerlink" title="对生活做种种设置是人特有的品性。不光是设置动物，也设置自己。我们知道，在古希腊有个斯巴达，那里的生活被设置得了无生趣，其目的就是要使男人成为亡命战士，使女人成为生育机器，前者像斗鸡，后者像母猪。这两类动物是很特别的，但我以为，它们肯定不喜欢自己的生活。但不喜欢又能怎么样？人也好，动物也罢，都很难改变自己的命运。 ————王小波 《一只特立独行的猪》"></a>对生活做种种设置是人特有的品性。不光是设置动物，也设置自己。我们知道，在古希腊有个斯巴达，那里的生活被设置得了无生趣，其目的就是要使男人成为亡命战士，使女人成为生育机器，前者像斗鸡，后者像母猪。这两类动物是很特别的，但我以为，它们肯定不喜欢自己的生活。但不喜欢又能怎么样？人也好，动物也罢，都很难改变自己的命运。 ————王小波 《一只特立独行的猪》</h5><p>距离上一次更新博客已经快五个月啦。这五个月发生了许许多多的意想不到的事情，有快乐的惊喜的、也有悲伤的委屈的。今日感冒请了病假在家休息，顺便回味一下这五个月以来的喜怒哀乐，以此博文作流水账。写完回看觉得这篇博客写得实在不好，还望各路大神多多指导一下！</p>
<h3 id="五月·隔离"><a href="#五月·隔离" class="headerlink" title="五月·隔离"></a>五月·隔离</h3><p><img src="https://res.cloudinary.com/dmfrqkuif/image/upload/v1603802357/IMG_7570_20200920-223800_lklvkp.jpg" alt=""><br>五月十号之前我在Hanover小镇过着悠闲的养老生活：寒研已经结束，小伙伴们全都已经离开，剩下可怜弱小又无助等机票的本人留守hanover，等待回国那一天的到来。</p>
<p><img src="https://res.cloudinary.com/dmfrqkuif/image/upload/v1603802406/IMG_8199_20200529-225733_snjrcl.jpg" alt=""><br>在广州隔离了十四天后，回到家里又被社区要求自觉居家隔离十四天，于是整个五月和六月初几乎都在隔离中度过了。现在回想起来，在广州隔离的那段日子还是非常的舒服的：热情的工作人员跑上跑下的给大家送外卖，温柔的护士小姐姐小哥哥每天上门查两次体温，隔离结束的时候隔离酒店还给每个人送了“福”的红底毛笔字留作纪念。在此再次感谢广州番禺区的宜必思酒店！</p>
<h3 id="六月·暑研"><a href="#六月·暑研" class="headerlink" title="六月·暑研"></a>六月·暑研</h3><p>这是一段难忘又难受的经历，大概就是跟着美国某著名高校的raising star一样的一个AP，然后八字不合闹的不欢而散的事情，在此略去一万个字。</p>
<h3 id="七月·保研"><a href="#七月·保研" class="headerlink" title="七月·保研"></a>七月·保研</h3><p>看着国际形势越来越恶化，以及父母的担心，我选择了保研。海投了一波学校：清华北大复旦上交中科大中科院港中文。中奖了港中文、中科大、中科院和清华。当时香港还没出国安法，担心局势不稳定就没去。清华的夏令营在中科大和中科院的前面，清华面试完之后自我感觉良好，所以就没有继续参加后面的两个学校的活动了。</p>
<p>虽然后面的录取过程非常的曲折，让我一度非常后悔当初为什么没多拿几个学校的offer。今年由于疫情的影响，教育部分发研究生名额的日期推迟了几乎一个月，因此我们学校的保研资格证明也迟迟没有发放，但是清华大学依旧催着要求我们出具保研资格证明或者强有力的辅助材料，由于本人实在是菜，以上三个机构的时间安排冲突给我造成了很大的麻烦，再次感谢帮助我的各位老师和给我舒缓压力的各位朋友。</p>
<h3 id="八月·手术"><a href="#八月·手术" class="headerlink" title="八月·手术"></a>八月·手术</h3><p>动了个小手术，住院了几天。 有了实习的念头，在educative上刷了一些算法题。刷的是grokking the coding interview的题目，我很喜欢这个课程，深入浅出，把算法题的套路分为几大类型，每个类型都先从简单的例子开始讲思想，然后再慢慢举一反三，给人一种无痛刷题的美好体验。<br><img src="https://res.cloudinary.com/dmfrqkuif/image/upload/v1603803523/%E6%8D%95%E8%8E%B7_nzyop8.png" alt=""></p>
<h3 id="九月·重返校园"><a href="#九月·重返校园" class="headerlink" title="九月·重返校园"></a>九月·重返校园</h3><p>年初一月离开武汉去美国的时候，武汉海鲜市场就已经爆出新闻说有不明传染病了，因此我还在宿舍买了两大包的口罩，虽然我也懒得戴，疫情期间送给宿管阿姨了。刚到美国不久武汉就封城了，再次回到校园已经是九月，这是我没有预料到的。</p>
<p>九月主要在折腾保研手续的事情，这个过程极其痛苦，好在最后顺利的保了研。九月还过了一次回想起来还是非常开心的生日。<br><img src="https://res.cloudinary.com/dmfrqkuif/image/upload/v1603803753/DF753D32318538769EEE485E29DFAAEF_ar1zra.jpg" alt=""></p>
<h3 id="十月·实习"><a href="#十月·实习" class="headerlink" title="十月·实习"></a>十月·实习</h3><p>投了三家公司，去了回复最快的那一家（对，就是Intel)。还有一家愿意要我的组是做游戏的，不太感兴趣就没去。还有一家回复的实在太慢，十月中旬才打电话来笔试(╬▔皿▔)凸</p>
<p>十月十四号，终于尘埃落地收到了清华大学的预录取通知。说不上多么开心，也说不上多么难受。不是很开心是因为原计划出国读书，恰好年初在Dartmouth度过了愉快的四个月，去完一趟美国之后更加憧憬在那里度过一段学习的时光了，最后没能去成，有一种曾经沧海难为水的感觉。但也不是很难受，研究生的导师出了名的好，师兄师姐也很热情，上周旁听了一次组会，组里氛围也很棒。那就随遇而安吧(<em>^_^</em>)<br><img src="https://res.cloudinary.com/dmfrqkuif/image/upload/v1603804400/IMG_1058_20201027-211306_vsrzgg.jpg" alt=""></p>
<p>十月二十二日，UIST会议在线上召开，寒研的论文终于发表出来啦！回想那段时光，又辛苦又开心。</p>
<p>9.28开始在上海intel实习。组里做quantize aware training的，来之前我对这个领域一无所知，只知道是机器学习优化、面试的时候觉得这个工作挺有意思的就来了。来之后日常懵逼，疯狂补概念。最近几天会再写一篇博文谈谈自己对quantize aware training的理解。实习遇到的人都很好，虽然他们不太会表达但是时间久了还是可以感受到大家满满的善意❤。感觉自己又菜又牛。目前仍然在适应实习的环境中，每天问自己无数次：“我是谁？我在哪？我要做什么？”。但还是时常感到茫然，开始怀疑人生：我是不是不适合做这种工作？我大学期间是不是过的太顺利了没遇到过一点挫折，才导致现在遇到挑战之后非常难受？有时候也会上升到开始思考人生的意义( ﹁ ﹁ ) ~→可能是我书读的太少，对于社会和心理的认知不够，才会陷入这样的迷茫吧。买了一些书，还是要多读书，尽量不要让自己陷在“书读的太少而想得太多”的困境中。</p>
<p>在上海租的小房子经过我一个月以来的收拾，变得非常的温馨啦。阳光洒进来，满屋子都是温暖的味道。<br><img src="https://res.cloudinary.com/dmfrqkuif/image/upload/v1603805430/E911F452FC56363BAD7FC64E944D825F_dfdzzj.jpg" alt=""></p>
<h3 id="碎碎念"><a href="#碎碎念" class="headerlink" title="碎碎念"></a>碎碎念</h3><p>仿佛困在围城中，晚安(￣o￣) . z Z</p>
]]></content>
      <tags>
        <tag>LIFE</tag>
      </tags>
  </entry>
  <entry>
    <title>Bye,2020</title>
    <url>/2020/12/31/Bye,2020/</url>
    <content><![CDATA[<p>2020年的最后一天~ 回忆一波即将过去的这一年，仅作留念。</p>
<h2 id="一月-出发"><a href="#一月-出发" class="headerlink" title="一月 出发"></a>一月 出发</h2><p><img src="https://res.cloudinary.com/dmfrqkuif/image/upload/v1609419704/04AA6BE66C672A42070A99C3AA4FB460_y0o4md.jpg" alt=""></p>
<p>1.12考完编译原理便直接去了上海，但是由于买的机票出了一点问题，一番波折后，1.15顺利的出发并到达了美国。飞越北极的时候在飞机上看到了让我非常震撼的风景。由于北极上空大气稀薄，可见度非常高，虽然人在飞机上、但是看到的下面的景色十分清晰，仿佛就在眼前。</p>
<p><img src="https://res.cloudinary.com/dmfrqkuif/image/upload/v1609419911/98B97EB5C9407E5AEBC3236F6D37F5B2_ovli2r.jpg" alt=""></p>
<p>Hanover小镇非常冷，第一次来到零下二十度的地方，又兴奋又害怕。</p>
<p><img src="https://res.cloudinary.com/dmfrqkuif/image/upload/v1609420086/00AF2330487430114D3C48BFD9A50949_qwu7np.jpg" alt=""></p>
<p>1.23 一个人在家里看了场春晚（惨兮兮。喵生第一次一个人过春节。此时的国内正是新冠非常严重的时候，到处想办法给家里买口罩。感谢LZM同学给我爸妈免费寄过去的口罩，真的非常感谢了！！！</p>
<h2 id="二月-寒研"><a href="#二月-寒研" class="headerlink" title="二月 寒研"></a>二月 寒研</h2><p><img src="https://res.cloudinary.com/dmfrqkuif/image/upload/v1609420165/E43DB56EF8121B3A3E6ECCE89280827A_nece3p.jpg" alt=""></p>
<p>达特茅斯非常美，在灿烂的阳光下，雪花慢慢飘落，走在厚厚的雪地上发出吱吱的声响。如果说去美国之前，出国读书对于我而言只是一个可有可无的选项，那么去了之后出国读书大概成了我梦寐以求的事情。</p>
<p><img src="https://res.cloudinary.com/dmfrqkuif/image/upload/v1609420442/D776AA09AAF608FCBF3594FD3ADC2296_pe8nud.jpg" alt=""></p>
<p>学校的冰雕节，图中的雕的是一个水怪，超可爱~</p>
<p><img src="https://res.cloudinary.com/dmfrqkuif/image/upload/v1609420342/879F87BF5F0ED6FFFE99601DB19E4A6D_bim34i.jpg" alt=""></p>
<p>寒研很有意思，我们用电容传感技术实现了织物上的非金属的物体识别。后来也成功的发表了论文在UIST上。</p>
<h2 id="三月-疫情"><a href="#三月-疫情" class="headerlink" title="三月 疫情"></a>三月 疫情</h2><p><img src="https://res.cloudinary.com/dmfrqkuif/image/upload/v1609420805/DEAB2084B9F9F51A8BF8E307A3DE5198_bczdm2.jpg" alt=""></p>
<p>三月，太阳开始往北回归线移动，天气转暖。此时的大毛依旧美丽，一派生机。</p>
<p>其实2019年12月武汉海鲜市场出现不明传染病的时候，我们学校的同学基本上都买了口罩并且有不少同学在图书馆戴了，但我万万没想到一个月之后这个疫情会以如此快的速度和杀伤力让中国几乎全部封城。更没想到的是三个月之后病毒越过大洋来到了美国。3.19学校宣布所有实验室必须关闭，于是我们把实验室从学校搬到了老板家里。</p>
<p><img src="https://res.cloudinary.com/dmfrqkuif/image/upload/v1609421026/8500cc5ec9f6a4fb61e1855e4831799_gcmgbt.jpg" alt="老板家的阳台，很美"></p>
<h2 id="四月-寒研收尾与告别"><a href="#四月-寒研收尾与告别" class="headerlink" title="四月 寒研收尾与告别"></a>四月 寒研收尾与告别</h2><p>此时实验基本上已经做完，论文在老板的n次要求下也已经改了无数遍，基本上就是在拍摄演示视频和添加老板要求的新功能。有时候会比较崩溃，明明自己觉得已经做到99分了，但是老板还是会提建议，希望你做到101分。但是事后回忆起来，却对老板非常感激。以前跟同学们打比赛也是，明明做的很不错了，但我还是会在DDL之前想着加各种东西或者把之前实现的东西打磨的再好一点，然后比赛下来结果都会很棒。发论文可能也是一样的道理，只有当自己的自我要求就已经很高了，希望自己做的完美完美再完美一些，这样会更容易得到审稿人的认可。</p>
<p><img src="https://res.cloudinary.com/dmfrqkuif/image/upload/v1609421255/409d86ecfd8c4ddc808964d50a192e9_nsxcqw.jpg" alt=""></p>
<p><img src="https://res.cloudinary.com/dmfrqkuif/image/upload/v1609421573/45084bc1622fe4a6abd39e440006068_qd4bfo.jpg" alt="我拍的演示图片之一"></p>
<p>另外一个感想就是，平时要尽可能的多学东西，技多不压身。比如论文中需要一些图片来直观地表明我们的作品的潜在功能，由于我平时有摄影的爱好，在色彩、光线、构图上的技巧就发挥了作用，拍下了很多好看的图片并放进了论文中。</p>
<p>不仅仅是业余兴趣要广泛为佳，在计算机方面的技能也要多多益善。不仅仅是自己做的这个小方向里面的东西，其他方面也要有所耳闻。比如这次寒研中需要快速开发一个手机APP，如果之前没有这方面的经验、等到需要用的时候再来学就有些仓促和痛苦。</p>
<p>研究生阶段想选做NLP方向，但是我认为CV甚至是分布式等其他方面的知识也是需要的。</p>
<p><img src="https://res.cloudinary.com/dmfrqkuif/image/upload/v1609422011/89e38d1b7d7a90eb328dfa58518dd1e_zpgahs.jpg" alt=""></p>
<p>四月底，hanover终于有了一点春天的迹象。依旧是非常的明媚、非常的美。</p>
<p><img src="https://res.cloudinary.com/dmfrqkuif/image/upload/v1609422089/1086b5185d1b5d241f3ed8ff154ed1a_nxe0pr.jpg" alt=""></p>
<p>那段时间的阳光都非常灿烂，相比之下好像那之后的一整年都暗淡无光。</p>
<p>UIST DDL之后，实验室的小伙伴几乎都回家或者投奔亲属去了，剩下俺一个人享受美国大农村的美丽春光了。</p>
<p><img src="https://res.cloudinary.com/dmfrqkuif/image/upload/v1609422262/b64d7ec2f9e90dcd7972988279b6f26_wbly4q.jpg" alt=""></p>
<p>美到落泪的connectedcut river！四月下旬到五月上旬，我的日常生活就是做做饭、追追剧、然后下午在河边跑跑步晒晒太阳。这段时间内，厨艺与体重共同进步。学会了做很多以前不会做的菜，不仅学会了煮米饭，还有做虾滑、做肥牛饭、牛排、煎好吃的三文鱼~</p>
<p><img src="https://res.cloudinary.com/dmfrqkuif/image/upload/v1609422369/7dd2588ccabfaa993a074b31dc14176_ovssdt.jpg" alt=""></p>
<p><img src="https://res.cloudinary.com/dmfrqkuif/image/upload/v1609422446/20df633db5016858053fc510dbb895d_fkmkvu.jpg" alt=""></p>
<h2 id="五月-回家-隔离"><a href="#五月-回家-隔离" class="headerlink" title="五月 回家+隔离"></a>五月 回家+隔离</h2><p><img src="https://res.cloudinary.com/dmfrqkuif/image/upload/v1609422624/907c83671f8ffe3533cc98f502caf37_mskrat.jpg" alt=""></p>
<p>春天到啦，大毛的草坪绿绿的，如果没有疫情五月应该在纽约西雅图浪了~实际上是在家里留守，倒计时回国的日子。 </p>
<p><img src="https://res.cloudinary.com/dmfrqkuif/image/upload/v1609422903/7469fee0e0dd51f077deccc3327e1a2_u1bkeq.jpg" alt=""><br>去美国的时候万万没想到会是自己一个人回去。回国飞机从洛杉矶出发，我所在的hanover到洛杉矶需要坐6-8小时的飞机，害怕中途飞机延误什么的，于是安排提前一天到机场附近住下，第二天再走。想到蓝可儿事件等等，我感到非常的害怕。。。还好一个人回去途中非常顺利，机场工作人员都很热情，离开hanover的时候，lebanon机场的可爱大叔还跟我开玩笑哈哈哈</p>
<h2 id="六月-期末-暑研"><a href="#六月-期末-暑研" class="headerlink" title="六月 期末+暑研"></a>六月 期末+暑研</h2><p>剩下最后一门必修的嵌入式的课，认真复习了几天就考完咯，好在简单。好像回家之后就没拍照片了哈哈哈</p>
<h2 id="七月-暑研"><a href="#七月-暑研" class="headerlink" title="七月 暑研"></a>七月 暑研</h2><p>之前找了一个美国的老师做暑研，这段时间就在陆陆续续的工作了。但是不知道为什么老师好像总是很忙很敷衍，中途还突然冒出来一个co-worker的博士哥哥。虽然博士哥哥人很好，但是这段科研还是很令人不爽，一段时间之后无法忍受就退出了。</p>
<p>这段经历的教训大概就是，任何东西都要宁缺毋滥。</p>
<p>七月家乡发生了很大的水灾，虽然没淹到我家这边，但是还是造成了很大的破坏，很多人的房子突然就塌了，还有很多人的商铺的商品来不及撤走就被泡坏了。对自然要有敬畏之心。</p>
<p><img src="https://res.cloudinary.com/dmfrqkuif/image/upload/v1609423642/dd781d537457bcc10cb64ab9f6ea607_h41txq.jpg" alt=""></p>
<p>随着美国的疫情变化以及中美关系的恶化，我从五月开始动摇的出国计划此时更加不坚定了。正值保研夏令营的申请季，申请也比较简单，网上填个报名表，有些稍微麻烦一点的需要寄纸质版材料，这些表格跟出国的时候的跟报纸一样字又小又多的申请表比起来简直不值一提，于是清北复交科以及港中文都申请了一下，中了港中文、清华深圳以及中科大。</p>
<h2 id="八月"><a href="#八月" class="headerlink" title="八月"></a>八月</h2><p><img src="https://res.cloudinary.com/dmfrqkuif/image/upload/v1609423851/351df567d3082925bf092bb1dabd2f1_e4urv0.jpg" alt=""></p>
<p>在家学会了做小蛋糕(●’◡’●)</p>
<p><img src="https://res.cloudinary.com/dmfrqkuif/image/upload/v1609423893/8f513656fb5ba9d0a398944116dff9f_paualk.jpg" alt=""></p>
<p>真·暑·假: 湖北景区全年免费了，于是这段时间跟家人出去玩~</p>
<p><img src="https://res.cloudinary.com/dmfrqkuif/image/upload/v1609423961/4ec1aa8f87b5506b3dbe3f75a5820ce_i5kinn.jpg" alt=""></p>
<p>住院做了一次小手术，打针好疼。。。体会到了健康的重要性，暗暗在心里立下了flag：出去之后一定要锻炼身体！（后面倒了）。这次手术还不小心把我的左眼整成了双眼皮，（但是右眼依然是单的）。如果说在2020的年末要许下什么愿望的话，我希望2021年我可以多多锻炼身体，跑跑步练练瑜伽什么的。</p>
<h2 id="九月-重返校园-约饭"><a href="#九月-重返校园-约饭" class="headerlink" title="九月 重返校园+约饭"></a>九月 重返校园+约饭</h2><p>疫情期间最受伤的武汉重新焕发出了生机，武汉大学也不例外。回到离别了八个月的大学，跟许久不见的朋友们约着到处吃吃喝喝玩玩。还考了驾照的科目一，之后就跑去上海实习了，希望明年毕业前能把驾照拿到，然后去西北自！驾！游！</p>
<p><img src="https://res.cloudinary.com/dmfrqkuif/image/upload/v1609424775/e39311c7634f0f5f4076c7957469e23_vqhrfe.jpg" alt=""></p>
<p><img src="https://res.cloudinary.com/dmfrqkuif/image/upload/v1609424985/1a7511e2151ed3f9b8f3c84a3e51bb2_i6xn1w.jpg" alt=""></p>
<p>保研名额迟迟不能确定下来，焦头烂额了一阵子。好在后来一切稳定。期间过了一次非常开心的生日o(<em>￣▽￣</em>)ブ</p>
<p><img src="https://res.cloudinary.com/dmfrqkuif/image/upload/v1609424990/4d30b0e666f47c5568cbd9a1481508a_pkpfz7.jpg" alt=""></p>
<p>大学期间的生日其实都挺开心的，记得大二的时候青协给我过了一次秘密生日也很开心，他们骗我说是开会，等我一到教室他们就把灯关掉，然后给我唱生日快乐歌，非常的惊喜~~</p>
<p>九月下旬拿到了intel的实习offer，于是去了上海开始实习咯</p>
<h2 id="十月-十二月-实习"><a href="#十月-十二月-实习" class="headerlink" title="十月-十二月 实习"></a>十月-十二月 实习</h2><p><img src="https://res.cloudinary.com/dmfrqkuif/image/upload/v1609425333/4ccf4047bdb9e0ed0700945ae628823_oeyilb.jpg" alt=""></p>
<p>来了上海才知道公司的位置这么偏。。。就是地铁坐到终点站之后还需要坐40分钟公交然后步行15分钟才能到公司。。。在公司附近没有找到合适的房子，找了一个稍微远但是离公司班车上车点近的地方，每天6：30起床去等班车。。。对于起床要命星人来说非常痛苦了。</p>
<p><img src="https://res.cloudinary.com/dmfrqkuif/image/upload/v1609425791/31f1dcd2c0d1a81df1adeb934fca7f1_jktatn.jpg" alt=""></p>
<p>租的小窝非常温馨,住的很满意~虽然房租非常贵，一个月4K，我的实习工资基本全都贡献给了房租。</p>
<p>实习并不顺利，一开始还很让人崩溃。我是一个不太爱问人很多问题的人，遇到问题先自己想、想不到的话就去google,还是想不到才会去问人。但是在公司，刚接触一个新的项目的时候是需要很勤快的多多问一些问题的，这与我之前的习惯产生了矛盾，结果就是我非常的痛苦和迷茫，不知道我是谁我在哪我在做什么。花了大概一个月，慢慢适应了去问同事们有关项目的问题，问一些我之前认为应该自己解决（其实解决不了）的问题。</p>
<p>在此感谢照顾我的同事们~ 坐在我后面的两个大哥哥都是今年刚毕业的研究生，交流起来很舒服（听得懂），他们也很照顾我，耐心的给我解释很多东西~</p>
<p><img src="https://res.cloudinary.com/dmfrqkuif/image/upload/v1609425717/3227af4a5a282d99978ac7671fec504_jlcpde.jpg" alt=""></p>
<p>10.22还线上召开了UIST<del>第一次参加学术会议，虽然是线上的但也是很激动</del></p>
<p>十一月和十二月依然是实习，本来准备另起一段，但好像这段时间没什么好讲的。主要工作就是做一些打杂的事情以及去调研一些新的技术和产品。最痛苦的其实是打杂，去修改别人的代码甚至给别人debug？？？ 虽然mentor和同事和老板都很照顾我，但是打杂还是有点无聊和难受≧ ﹏ ≦。 调研新技术和产品倒是比较有趣，期间接触了量化压缩模型的各种方法，可解释AI、自动生成CUDA代码的方法，以及神经网络中的自动调参。以前只觉得NLP有趣，现在会觉得模型压缩、可解释AI和自动调参都还很有趣。</p>
<p>通过这段实习，大概知道了inel这样的不用996的外企里面程序员的真实生活节奏。观察和体验工业界程序员的生活其实是这次实习的主要目的。我所在的组是intel不那么养老的、比较忙的组。一个月一次code freeze, code freeze 前后比较忙，会有加班的情况。带我的mentor非常肝，不论是晚上还是周末，我每次上skype的时候mentor都是在线的。mentor是组里的技术leader，技术确实非常厉害，有此找他问问题，只见mentor飞快的在键盘上操作vim，然后bug就解决掉了。mentor 可能是我目前见过的最厉害的程序员，一个mentor可以抵100个我这样的萌新~</p>
<p>虽然很佩服和崇拜mentor，但是那似乎并不是我想要的生活，并不想成为一个很厉害的程序员，可能更喜欢在达特茅斯的实验室跟着小伙伴们一起开脑洞、做实验的时光~ 或者是更喜欢在武大带着队友们心怀创业梦想做magic mirror的时候。人的一生真的好短暂，希望能够快快找到自己喜欢的事情然后把它做的好一些(●’◡’●)</p>
<p>技术总结在这里：<br><a href="https://zhuanlan.zhihu.com/p/340578370" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/340578370</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/340574364" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/340574364</a></p>
<p><img src="https://res.cloudinary.com/dmfrqkuif/image/upload/v1609426318/20ea8ad9b03fab4233054c602f8406f_s2ypsu.jpg" alt=""></p>
<p>实习结束后去了一趟迪士尼，真的超级好玩了~ 里面的项目老少皆宜，比较有趣又不是那么刺激吓人，深得我这个养老玩家的心~</p>
<h4 id="大概花了两个小时写完这篇没打草稿也没构思的文章，2020还有一个小时就要过去，祝家人朋友们新年快乐！"><a href="#大概花了两个小时写完这篇没打草稿也没构思的文章，2020还有一个小时就要过去，祝家人朋友们新年快乐！" class="headerlink" title="大概花了两个小时写完这篇没打草稿也没构思的文章，2020还有一个小时就要过去，祝家人朋友们新年快乐！"></a>大概花了两个小时写完这篇没打草稿也没构思的文章，2020还有一个小时就要过去，祝家人朋友们新年快乐！</h4>]]></content>
      <tags>
        <tag>LIFE</tag>
      </tags>
  </entry>
  <entry>
    <title>在家磕盐杂记</title>
    <url>/2021/03/12/%E5%9C%A8%E5%AE%B6%E7%A3%95%E7%9B%90%E6%9D%82%E8%AE%B0/</url>
    <content><![CDATA[<p>2021已经过去两个多月，这个冬天与往年格外不同：失去了最亲爱的奶奶，开始思考以前总是逃避的生死话题；第一次一个人成功选好了主题，一个人读论文、写代码、做实验……以及人生第一次崴伤了脚，刚到学校的我被迫再次回家磕盐；开始喜欢看人文类的书，意识到自己高中选完理科以来似乎就没好好看过书、人文方面的知识极度匮乏。</p>
<h2 id="关于生死"><a href="#关于生死" class="headerlink" title="关于生死"></a>关于生死</h2><p>小时候就很害怕看鬼片，更不敢去参加葬礼，在农村看到坟墓就赶紧离的远远的……过去这二十多年，我一直害怕死亡、逃避与死亡有关的一切、更不敢去思考这个问题。但是今年，奶奶在和肺癌抗争了几年后，还是肺衰竭去世了。一直哭，从下午哭到晚上，哭到第二天……最疼爱我的奶奶就这样离去了，再也无法给她带好吃的小零食、再也无法跟她一起回忆我小时候的可可爱爱的事情，以后回到家再也没有奶奶可以叫了……想念奶奶♥</p>
<p>冷静下来过后，只能自我安慰，接受人的有限性。我们总是祝福万事如意，事事顺心，长命百岁等等，但实际上我们可以控制和改变的事情太少了，生老病死都是我们无法控制的。除了接受事实以外，我们似乎没有任何其他的选择。</p>
<p>小学时读史铁生的《我与地坛》，对一句话印象深刻，总感觉写的很有哲理、又似乎看不懂：</p>
<blockquote>
<blockquote>
<p>“但是太阳，它每时每刻都是夕阳也是旭日，当它熄灭着走下山去收尽苍凉残照之际，正是它在另一面燃烧着爬上山巅布散烈烈朝辉之时。那一天，我也将沉静着走下山去，扶着我的拐杖。有一天，在某一处山洼里，势必会跑上来一个欢蹦的孩子，抱着他的玩具。<br>当然，那不是我。<br>但是，那不是我吗？<br>宇宙以其不息的欲望将一个歌舞炼为永恒。这欲望有怎样一个人间的姓名，大可忽略不计。”</p>
</blockquote>
</blockquote>
<p>现在再读，才发现史铁生把生死轮回写的如此轻松洒脱，实在佩服。</p>
<p>以前总想干大事，想叱咤风云、想创业搞个大事情，现在不那么坚定了，每天跟家人在一起开开心心幸幸福福的一生也足矣。</p>
<h2 id="SOLO磕盐是什么样的体验"><a href="#SOLO磕盐是什么样的体验" class="headerlink" title="SOLO磕盐是什么样的体验"></a>SOLO磕盐是什么样的体验</h2><p>初选题时：Genius, awesome, perfect!</p>
<p>开始调研这个课题目前的发展情况：loop(他们好菜—我菜爆了—平静下来搞事情)</p>
<p>最开始做课题的时候非常怀念在Dartmouth的时光，有Phd带，有大Boss撑腰，还有其他可可爱爱的intern在一起相互鼓励，每天都信心满满开开心心。而自己一个人在家做课题，就时常自我怀疑，尤其是遇到困难的时候最甚。还好导师超级nice，给了我很多鼓励。还有有爱的家人以及妈妈做的超好吃的菜，每天吃饱了都觉得自己满血复活233</p>
<p>最近状态还不错，走的比较稳，验证想法的时候先在简单任务上试试再去做大的，稳扎稳打比较踏实；另一方面是备好了plan B和plan CDEFG…… 总之目前正在稳稳的推进项目！</p>
<h2 id="书中自有黄金屋"><a href="#书中自有黄金屋" class="headerlink" title="书中自有黄金屋"></a>书中自有黄金屋</h2><p>小学和初中的时候超喜欢看书，小学喜欢看外国人写的杂文以及心灵鸡汤（误入奇奇怪怪的东西），中学喜欢看散文以及言情小说（误入++），高中由于班主任的打击没看什么，现在慢慢又萌生出看书的想法。</p>
<p>最近喜欢看书可能是受到罗翔老师的影响，2020年跨年看吐槽大会跨年晚会时第一次见到罗翔老师，被罗老师的知识所吸引和感动。恰好又跟同学逛到了大隐书局、静安书店等有文化的好书店，发现了一些想要看的书，遂回家后开始捡起看书的习惯了。</p>
<p>寒假看的是叔本华的《人生的智慧》。这本书中，叔本华像是一个长者一样娓娓道来，在人的自身、财产、人所展现的表现、人生的各个阶段这些方面给出了自己的思考，还有一章是建议与格言。有朋友把这本书称为高级的心灵鸡汤，我也认可233. 随意摘几段：</p>
<blockquote>
<blockquote>
<p>我们的幸福是多么的取决于我们的自身，即取决于我们的个性。但我们通常却是只是考虑运气，只是考虑所拥有的财产，或者我们在他人心目中的样子。其实，运气会有变好的时候；再者，如果我们内在丰富的话，就不会对运气有太多的要求。</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>出自完全健康和良好体魄的宁静与愉快的脾性，清晰、活跃、深刻和正确的理解力，温和、节制有度的意欲以及由此产生的清白良心————所有这些好处，是任何地位、财富都不能代替的。</p>
</blockquote>
</blockquote>
<p>晚安~</p>
]]></content>
      <tags>
        <tag>LIFE</tag>
      </tags>
  </entry>
</search>
